# ðŸš€ Complete Deployment Guide: Pakistan Sales Data Analysis Project

## ðŸ“‹ Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture & Components](#architecture--components)
3. [Prerequisites](#prerequisites)
4. [Step-by-Step Deployment](#step-by-step-deployment)
5. [Data Loading & ETL](#data-loading--etl)
6. [Streamlit Application Deployment](#streamlit-application-deployment)
7. [Verification & Testing](#verification--testing)
8. [Troubleshooting](#troubleshooting)
9. [Maintenance & Monitoring](#maintenance--monitoring)

---

## ðŸŽ¯ Project Overview

This project implements a complete **OLTP â†’ OLAP â†’ Data Science** pipeline for Pakistan Sales Data Analysis:

- **OLTP Database**: Transactional data (3NF normalized)
- **OLAP Data Warehouse**: Analytical data (Star schema)
- **ETL Pipeline**: Data transformation and loading
- **Streamlit Dashboard**: Interactive analytics and ML insights
- **Snowflake Cloud**: Complete cloud implementation

---

## ðŸ—ï¸ Architecture & Components

### Database Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Files     â”‚    â”‚   OLTP DB       â”‚    â”‚   OLAP DWH      â”‚
â”‚   (Staging)     â”‚â”€â”€â”€â–¶â”‚   (3NF)         â”‚â”€â”€â”€â–¶â”‚   (Star Schema) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   ETL Process   â”‚    â”‚   Streamlit     â”‚
                       â”‚   (Python/SQL)  â”‚    â”‚   Dashboard     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Structure
```
pakistan-sales-data-analysis/
â”œâ”€â”€ 01_oltp_database_setup.sql          # OLTP database creation
â”œâ”€â”€ 02_data_loading_oltp.sql            # Data loading into OLTP
â”œâ”€â”€ 03_olap_data_warehouse_setup.sql    # OLAP data warehouse creation
â”œâ”€â”€ 04_etl_oltp_to_olap.sql            # ETL from OLTP to OLAP
â”œâ”€â”€ streamlit_dashboard.py               # Streamlit application
â”œâ”€â”€ streamlit_requirements.txt           # Python dependencies
â”œâ”€â”€ pakistan_sales_data/                 # Generated CSV data
â”‚   â”œâ”€â”€ pakistan_sales_data.csv
â”‚   â”œâ”€â”€ pakistan_customers.csv
â”‚   â”œâ”€â”€ pakistan_products.csv
â”‚   â””â”€â”€ ... (other CSV files)
â””â”€â”€ COMPLETE_DEPLOYMENT_GUIDE.md        # This guide
```

---

## âœ… Prerequisites

### 1. Snowflake Account
- **Account**: Active Snowflake account with admin privileges
- **Region**: Any Snowflake region (AWS, Azure, GCP)
- **Edition**: Enterprise or higher recommended

### 2. Snowflake User Setup
```sql
-- Create dedicated user for the project
CREATE USER PAKISTAN_SALES_USER 
    PASSWORD = 'YourSecurePassword123!'
    LOGIN_NAME = 'pakistan_sales_user'
    DISPLAY_NAME = 'Pakistan Sales Data Analyst'
    FIRST_NAME = 'Data'
    LAST_NAME = 'Analyst'
    EMAIL = 'analyst@company.com'
    MUST_CHANGE_PASSWORD = FALSE;

-- Grant necessary roles
GRANT ROLE ACCOUNTADMIN TO USER PAKISTAN_SALES_USER;
```

### 3. Local Environment
- **Python**: 3.8+ installed
- **Git**: For version control
- **Text Editor**: VS Code, Sublime, or similar
- **Terminal**: Command prompt or PowerShell

### 4. Python Dependencies
```bash
# Install required packages
pip install -r streamlit_requirements.txt
```

---

## ðŸš€ Step-by-Step Deployment

### Phase 1: OLTP Database Setup

#### Step 1.1: Connect to Snowflake
```bash
# Using SnowSQL CLI
snowsql -a your_account -u your_username -d your_database

# Or use Snowflake Web Interface
# Navigate to: https://your_account.snowflakecomputing.com
```

#### Step 1.2: Execute OLTP Setup
```sql
-- Run the complete OLTP setup script
-- This creates database, schemas, tables, and warehouses
SOURCE 01_oltp_database_setup.sql;
```

**Expected Output:**
```
âœ… Database: PAKISTAN_SALES_OLTP_DB created
âœ… Schemas: STAGING, OLTP, AUDIT created
âœ… Warehouses: PAKISTAN_OLTP_WH, PAKISTAN_LOADING_WH created
âœ… Tables: 8 normalized tables created
âœ… Indexes: Performance indexes created
âœ… Permissions: Access control configured
```

#### Step 1.3: Verify OLTP Setup
```sql
-- Check database creation
SHOW DATABASES LIKE 'PAKISTAN_SALES_OLTP_DB';

-- Check schemas
SHOW SCHEMAS IN DATABASE PAKISTAN_SALES_OLTP_DB;

-- Check tables
SHOW TABLES IN SCHEMA PAKISTAN_SALES_OLTP_DB.OLTP;
```

### Phase 2: Data Loading into OLTP

#### Step 2.1: Upload CSV Files
```sql
-- Switch to staging schema
USE SCHEMA PAKISTAN_SALES_OLTP_DB.STAGING;

-- Upload CSV files to stage
PUT file://pakistan_sales_data/pakistan_customers.csv @STAGING.CSV_STAGE;
PUT file://pakistan_sales_data/pakistan_customer_addresses.csv @STAGING.CSV_STAGE;
PUT file://pakistan_sales_data/pakistan_product_categories.csv @STAGING.CSV_STAGE;
PUT file://pakistan_sales_data/pakistan_products.csv @STAGING.CSV_STAGE;
PUT file://pakistan_sales_data/pakistan_stores.csv @STAGING.CSV_STAGE;
PUT file://pakistan_sales_data/pakistan_employees.csv @STAGING.CSV_STAGE;
PUT file://pakistan_sales_data/pakistan_sales_data.csv @STAGING.CSV_STAGE;
```

#### Step 2.2: Execute Data Loading
```sql
-- Run the data loading script
SOURCE 02_data_loading_oltp.sql;
```

**Expected Output:**
```
âœ… CSV files uploaded to staging area
âœ… Data loaded into staging tables with validation
âœ… Data quality checks performed
âœ… Data transformed and loaded into OLTP tables
âœ… Referential integrity maintained
âœ… Verification queries executed
```

#### Step 2.3: Verify Data Loading
```sql
-- Check record counts
SELECT 'OLTP CUSTOMERS' as TABLE_NAME, COUNT(*) as RECORD_COUNT 
FROM PAKISTAN_SALES_OLTP_DB.OLTP.CUSTOMERS
UNION ALL
SELECT 'OLTP PRODUCTS', COUNT(*) FROM PAKISTAN_SALES_OLTP_DB.OLTP.PRODUCTS
UNION ALL
SELECT 'OLTP ORDERS', COUNT(*) FROM PAKISTAN_SALES_OLTP_DB.OLTP.ORDERS;
```

### Phase 3: OLAP Data Warehouse Setup

#### Step 3.1: Execute OLAP Setup
```sql
-- Run the complete OLAP setup script
-- This creates analytical database with star schema
SOURCE 03_olap_data_warehouse_setup.sql;
```

**Expected Output:**
```
âœ… Database: PAKISTAN_SALES_OLAP_DB created
âœ… Schemas: DIMENSIONS, FACTS, AGGREGATES, ANALYTICS created
âœ… Warehouses: PAKISTAN_OLAP_WH, PAKISTAN_ETL_WH created
âœ… Dimension Tables: 7 dimension tables created
âœ… Fact Tables: 3 fact tables created
âœ… Aggregate Tables: 3 aggregate tables created
âœ… Star Schema: Complete with proper relationships
```

#### Step 3.2: Verify OLAP Setup
```sql
-- Check OLAP database
SHOW DATABASES LIKE 'PAKISTAN_SALES_OLAP_DB';

-- Check dimension tables
SHOW TABLES IN SCHEMA PAKISTAN_SALES_OLAP_DB.DIMENSIONS;

-- Check fact tables
SHOW TABLES IN SCHEMA PAKISTAN_SALES_OLAP_DB.FACTS;
```

### Phase 4: ETL Pipeline Execution

#### Step 4.1: Execute ETL Process
```sql
-- Run the complete ETL script
-- This transforms and loads data from OLTP to OLAP
SOURCE 04_etl_oltp_to_olap.sql;
```

**Expected Output:**
```
âœ… Time dimension populated with comprehensive date attributes
âœ… Payment and shipping method dimensions populated
âœ… Customer dimension populated with denormalized attributes
âœ… Product dimension populated with performance metrics
âœ… Store dimension populated with location and performance data
âœ… Employee dimension populated with hierarchy and performance data
âœ… Main sales fact table populated with transactional data
âœ… Daily sales summary fact table populated
âœ… Customer behavior fact table populated with RFM analysis
âœ… Aggregate tables populated for performance optimization
âœ… Calculated fields updated in all dimension tables
âœ… Data quality verification completed
```

#### Step 4.2: Verify ETL Results
```sql
-- Check OLAP data population
SELECT 'OLAP DIM_TIME' as TABLE_NAME, COUNT(*) as RECORD_COUNT 
FROM PAKISTAN_SALES_OLAP_DB.DIMENSIONS.DIM_TIME
UNION ALL
SELECT 'OLAP FACT_SALES', COUNT(*) FROM PAKISTAN_SALES_OLAP_DB.FACTS.FACT_SALES
UNION ALL
SELECT 'OLAP AGG_MONTHLY_SALES', COUNT(*) FROM PAKISTAN_SALES_OLAP_DB.AGGREGATES.AGG_MONTHLY_SALES;
```

---

## ðŸ“Š Data Loading & ETL

### Data Flow Summary
```
CSV Files â†’ Staging Tables â†’ OLTP Tables â†’ OLAP Dimensions/Facts â†’ Aggregates
    â†“              â†“              â†“              â†“                    â†“
Validation    Transformation   Normalization   Denormalization   Pre-computation
```

### Key ETL Features
- **Data Validation**: Null checks, duplicate detection, referential integrity
- **Data Transformation**: Type casting, data cleansing, calculated fields
- **Performance Optimization**: Clustering, indexing, materialized views
- **Audit Trail**: Complete logging of all data operations

### ETL Monitoring
```sql
-- Check ETL audit logs
SELECT * FROM PAKISTAN_SALES_OLTP_DB.AUDIT.DATA_LOAD_AUDIT
ORDER BY START_TIME DESC;

-- Check data quality metrics
SELECT * FROM PAKISTAN_SALES_OLTP_DB.AUDIT.DATA_QUALITY_AUDIT
ORDER BY CHECK_TIMESTAMP DESC;
```

---

## ðŸŒ Streamlit Application Deployment

### Option 1: Local Deployment

#### Step 1: Environment Setup
```bash
# Create virtual environment
python -m venv pakistan_sales_env
source pakistan_sales_env/bin/activate  # On Windows: pakistan_sales_env\Scripts\activate

# Install dependencies
pip install -r streamlit_requirements.txt
```

#### Step 2: Configuration
Create `.streamlit/secrets.toml`:
```toml
[snowflake]
user = "your_username"
password = "your_password"
account = "your_account"
warehouse = "PAKISTAN_OLAP_WH"
database = "PAKISTAN_SALES_OLAP_DB"
schema = "FACTS"
```

#### Step 3: Run Application
```bash
# Start Streamlit
streamlit run streamlit_dashboard.py

# Application will open at: http://localhost:8501
```

### Option 2: Cloud Deployment (Streamlit Cloud)

#### Step 1: Push to GitHub
```bash
# Initialize git repository
git init
git add .
git commit -m "Initial commit: Pakistan Sales Analytics Dashboard"
git branch -M main
git remote add origin https://github.com/yourusername/pakistan-sales-analytics.git
git push -u origin main
```

#### Step 2: Deploy on Streamlit Cloud
1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Connect your GitHub account
3. Select your repository
4. Set main file path: `streamlit_dashboard.py`
5. Add secrets in the Streamlit Cloud dashboard
6. Deploy!

### Option 3: Docker Deployment

#### Step 1: Create Dockerfile
```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "streamlit_dashboard.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

#### Step 2: Build and Run
```bash
# Build image
docker build -t pakistan-sales-dashboard .

# Run container
docker run -p 8501:8501 pakistan-sales-dashboard
```

---

## âœ… Verification & Testing

### 1. Database Verification
```sql
-- OLTP Verification
USE DATABASE PAKISTAN_SALES_OLTP_DB;
SELECT COUNT(*) FROM OLTP.CUSTOMERS;        -- Should be ~2,000
SELECT COUNT(*) FROM OLTP.PRODUCTS;         -- Should be ~800
SELECT COUNT(*) FROM OLTP.ORDERS;           -- Should be ~10,000

-- OLAP Verification
USE DATABASE PAKISTAN_SALES_OLAP_DB;
SELECT COUNT(*) FROM DIMENSIONS.DIM_CUSTOMER;    -- Should be ~2,000
SELECT COUNT(*) FROM FACTS.FACT_SALES;           -- Should be ~29,000+
SELECT COUNT(*) FROM AGGREGATES.AGG_MONTHLY_SALES; -- Should be ~24
```

### 2. Data Quality Checks
```sql
-- Check for null values in critical fields
SELECT COUNT(*) FROM PAKISTAN_SALES_OLTP_DB.OLTP.CUSTOMERS 
WHERE FIRST_NAME IS NULL OR LAST_NAME IS NULL;

-- Check referential integrity
SELECT COUNT(*) FROM PAKISTAN_SALES_OLTP_DB.OLTP.ORDERS o
LEFT JOIN PAKISTAN_SALES_OLTP_DB.OLTP.CUSTOMERS c ON o.CUSTOMER_ID = c.CUSTOMER_ID
WHERE c.CUSTOMER_ID IS NULL;
```

### 3. Performance Testing
```sql
-- Test query performance
SELECT 
    s.PRIMARY_PROVINCE,
    COUNT(DISTINCT c.CUSTOMER_KEY) as CUSTOMER_COUNT,
    SUM(cb.TOTAL_SPENT) as TOTAL_SALES
FROM PAKISTAN_SALES_OLAP_DB.DIMENSIONS.DIM_CUSTOMER c
JOIN PAKISTAN_SALES_OLAP_DB.FACTS.FACT_CUSTOMER_BEHAVIOR cb 
    ON c.CUSTOMER_KEY = cb.CUSTOMER_KEY
GROUP BY s.PRIMARY_PROVINCE
ORDER BY TOTAL_SALES DESC;
```

### 4. Streamlit Application Testing
- **Dashboard Loading**: Verify all charts and metrics display correctly
- **Data Filtering**: Test province, segment, and date filters
- **Interactive Elements**: Verify chart interactions and drill-downs
- **Performance**: Check response times for data loading

---

## ðŸ”§ Troubleshooting

### Common Issues & Solutions

#### 1. Snowflake Connection Issues
**Problem**: Cannot connect to Snowflake
**Solution**: 
```bash
# Check credentials
snowsql -a your_account -u your_username -d your_database

# Verify account format
# Correct: xy12345.us-east-1
# Incorrect: xy12345.us-east-1.snowflakecomputing.com
```

#### 2. Data Loading Errors
**Problem**: CSV files fail to load
**Solution**:
```sql
-- Check file format
DESC FILE FORMAT STAGING.CSV_FORMAT;

-- Verify file upload
LIST @STAGING.CSV_STAGE;

-- Check file encoding (should be UTF-8)
-- Re-upload files if necessary
```

#### 3. ETL Process Failures
**Problem**: ETL script fails during execution
**Solution**:
```sql
-- Check warehouse status
SHOW WAREHOUSES LIKE 'PAKISTAN_ETL_WH';

-- Resume warehouse if suspended
ALTER WAREHOUSE PAKISTAN_ETL_WH RESUME;

-- Check error logs
SELECT * FROM PAKISTAN_SALES_OLTP_DB.AUDIT.DATA_LOAD_AUDIT
WHERE LOAD_STATUS = 'FAILED';
```

#### 4. Streamlit Connection Issues
**Problem**: Dashboard cannot connect to Snowflake
**Solution**:
```python
# Check secrets configuration
import streamlit as st
st.write(st.secrets["snowflake"])

# Verify connection parameters
# Test connection manually
import snowflake.connector
conn = snowflake.connector.connect(
    user=st.secrets["snowflake"]["user"],
    password=st.secrets["snowflake"]["password"],
    account=st.secrets["snowflake"]["account"]
)
```

#### 5. Performance Issues
**Problem**: Slow query performance
**Solution**:
```sql
-- Check warehouse size
SHOW WAREHOUSES LIKE 'PAKISTAN_OLAP_WH';

-- Increase warehouse size if needed
ALTER WAREHOUSE PAKISTAN_OLAP_WH SET WAREHOUSE_SIZE = 'MEDIUM';

-- Check clustering status
SELECT * FROM TABLE(INFORMATION_SCHEMA.CLUSTERING_INFORMATION(
    'PAKISTAN_SALES_OLAP_DB.FACTS.FACT_SALES'
));
```

---

## ðŸ› ï¸ Maintenance & Monitoring

### 1. Regular Maintenance Tasks

#### Daily
- Monitor warehouse usage and costs
- Check for failed ETL processes
- Verify data freshness

#### Weekly
- Review performance metrics
- Check storage usage
- Validate data quality

#### Monthly
- Update clustering keys if needed
- Review and optimize warehouse sizes
- Archive old audit logs

### 2. Monitoring Queries

#### Warehouse Usage
```sql
-- Monitor warehouse usage
SELECT 
    WAREHOUSE_NAME,
    START_TIME,
    END_TIME,
    CREDITS_USED,
    BYTES_SCANNED,
    PERCENTAGE_SCANNED_FROM_CACHE
FROM TABLE(INFORMATION_SCHEMA.WAREHOUSE_METERING_HISTORY(
    DATE_RANGE_START => DATEADD('day', -7, CURRENT_DATE()),
    DATE_RANGE_END => CURRENT_DATE()
))
ORDER BY START_TIME DESC;
```

#### Data Quality Monitoring
```sql
-- Check data quality metrics
SELECT 
    TABLE_NAME,
    CHECK_TYPE,
    RECORDS_CHECKED,
    RECORDS_FAILED,
    CHECK_STATUS,
    CHECK_TIMESTAMP
FROM PAKISTAN_SALES_OLTP_DB.AUDIT.DATA_QUALITY_AUDIT
WHERE CHECK_TIMESTAMP >= DATEADD('day', -1, CURRENT_DATE())
ORDER BY CHECK_TIMESTAMP DESC;
```

#### Performance Monitoring
```sql
-- Monitor query performance
SELECT 
    QUERY_TEXT,
    START_TIME,
    END_TIME,
    TOTAL_ELAPSED_TIME,
    BYTES_SCANNED,
    ROWS_PRODUCED
FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY())
WHERE START_TIME >= DATEADD('hour', -24, CURRENT_TIMESTAMP())
ORDER BY TOTAL_ELAPSED_TIME DESC
LIMIT 10;
```

### 3. Automated Maintenance

#### Create Maintenance Tasks
```sql
-- Task for refreshing materialized views
CREATE OR REPLACE TASK REFRESH_ANALYTICS
    WAREHOUSE = PAKISTAN_ETL_WH
    SCHEDULE = 'USING CRON 0 2 * * * UTC'  -- Daily at 2 AM UTC
AS
BEGIN
    -- Refresh customer analytics
    CALL PAKISTAN_SALES_OLAP_DB.ANALYTICS.REFRESH_CUSTOMER_ANALYTICS();
    
    -- Refresh product analytics
    CALL PAKISTAN_SALES_OLAP_DB.ANALYTICS.REFRESH_PRODUCT_ANALYTICS();
    
    -- Refresh store analytics
    CALL PAKISTAN_SALES_OLAP_DB.ANALYTICS.REFRESH_STORE_ANALYTICS();
END;

-- Resume task
ALTER TASK REFRESH_ANALYTICS RESUME;
```

### 4. Cost Optimization

#### Warehouse Management
```sql
-- Set auto-suspend for cost optimization
ALTER WAREHOUSE PAKISTAN_OLTP_WH SET AUTO_SUSPEND = 60;
ALTER WAREHOUSE PAKISTAN_OLAP_WH SET AUTO_SUSPEND = 300;
ALTER WAREHOUSE PAKISTAN_ETL_WH SET AUTO_SUSPEND = 600;

-- Set credit limits
ALTER WAREHOUSE PAKISTAN_OLTP_WH SET CREDIT_QUOTA = 100;
ALTER WAREHOUSE PAKISTAN_OLAP_WH SET CREDIT_QUOTA = 200;
ALTER WAREHOUSE PAKISTAN_ETL_WH SET CREDIT_QUOTA = 300;
```

---

## ðŸŽ‰ Success Criteria

### Deployment Complete When:
âœ… **OLTP Database**: All tables created and populated with data  
âœ… **OLAP Data Warehouse**: Star schema implemented with dimensions and facts  
âœ… **ETL Pipeline**: Data successfully transformed from OLTP to OLAP  
âœ… **Streamlit Dashboard**: Application running and displaying data correctly  
âœ… **Data Quality**: All validation checks passing  
âœ… **Performance**: Queries executing within acceptable time limits  
âœ… **Monitoring**: Audit logs and performance metrics available  

### Expected Results:
- **OLTP Records**: ~45,000+ records across all tables
- **OLAP Records**: ~29,000+ fact records with 7 dimensions
- **Dashboard Response**: <5 seconds for data loading
- **Query Performance**: <10 seconds for complex analytical queries

---

## ðŸ“š Additional Resources

### Documentation
- [Snowflake Documentation](https://docs.snowflake.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Plotly Documentation](https://plotly.com/python/)

### Best Practices
- **Data Modeling**: Follow star schema principles for OLAP
- **Performance**: Use appropriate warehouse sizes and clustering
- **Security**: Implement role-based access control
- **Monitoring**: Set up comprehensive logging and alerting

### Support
- **Snowflake Support**: Available through your account
- **Community Forums**: Snowflake and Streamlit communities
- **GitHub Issues**: For code-related problems

---

## ðŸš€ Quick Start Commands

```bash
# 1. Clone repository
git clone https://github.com/yourusername/pakistan-sales-analytics.git
cd pakistan-sales-analytics

# 2. Install dependencies
pip install -r streamlit_requirements.txt

# 3. Configure Snowflake connection
# Edit .streamlit/secrets.toml with your credentials

# 4. Deploy databases (run in Snowflake)
SOURCE 01_oltp_database_setup.sql;
SOURCE 02_data_loading_oltp.sql;
SOURCE 03_olap_data_warehouse_setup.sql;
SOURCE 04_etl_oltp_to_olap.sql;

# 5. Run Streamlit dashboard
streamlit run streamlit_dashboard.py
```

---

**ðŸŽ¯ Project Status**: Ready for Production Deployment  
**ðŸ“… Last Updated**: December 2024  
**ðŸ”§ Version**: 1.0.0  
**ðŸ‘¥ Maintainer**: Data Engineering Team  

---

*This guide covers the complete deployment of the Pakistan Sales Data Analysis Project. Follow each step carefully to ensure successful implementation.*
